import os
import json
import yaml
import sys
from openai import OpenAI
from dotenv import load_dotenv
from .models import ResumeFull, ResumeCritique

load_dotenv()

class PromptManager:
    """Prompt ä»“åº“ç®¡ç†ç±»"""
    def __init__(self, prompt_file: str = "default.yaml"):
        current_dir = os.path.dirname(os.path.abspath(__file__))
        file_path = os.path.join(current_dir, "prompts", prompt_file)
        with open(file_path, "r", encoding="utf-8") as f:
            self.prompts = yaml.safe_load(f)

    def get_draft_prompt(self) -> str:
        p = self.prompts
        return f"{p['role_definition']}\n{p['core_principles']}\n{p['field_guide']}"

    def get_critic_prompt(self) -> str:
        return self.prompts['critic_instruction']

    def get_refine_prompt(self) -> str:
        return self.prompts['refine_instruction']

class ResumeAgent:
    def __init__(self, model: str = "deepseek-chat"):
        self.client = OpenAI(
            api_key=os.getenv("OPENAI_API_KEY"),
            base_url=os.getenv("OPENAI_BASE_URL", "https://api.deepseek.com")
        )
        self.model = model
        self.prompt_manager = PromptManager()
        
    def _print_status(self, message: str, color: str = "\033[94m"):
        """ç®€æ´çš„å½©è‰²çŠ¶æ€è¾“å‡º"""
        print(f"{color}{message}\033[0m")
        sys.stdout.flush()

    def _call_llm(self, system_prompt: str, user_prompt: str, response_model):
        try:
            schema_json = json.dumps(response_model.model_json_schema(), ensure_ascii=False)
            messages = [
                {"role": "system", "content": f"{system_prompt}\n\nè¯·ä¸¥æ ¼æŒ‰æ­¤ JSON Schema è¾“å‡º:\n{schema_json}"},
                {"role": "user", "content": user_prompt}
            ]
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                response_format={"type": "json_object"}
            )
            
            raw_content = response.choices[0].message.content
            return response_model.model_validate_json(raw_content)
        except Exception as e:
            self._print_status(f"âŒ LLM è°ƒç”¨å¼‚å¸¸: {str(e)}", "\033[91m")
            raise e

    def tailor(self, raw_thoughts: str, jd_text: str) -> ResumeFull:
        """æ ¸å¿ƒæµç¨‹ï¼šDraft -> Critic -> Refine"""
        
        # --- Phase 1: Draft ---
        self._print_status("âœï¸  æ­£åœ¨èµ·è‰ç®€å†åˆç¨¿...")
        draft_prompt = self.prompt_manager.get_draft_prompt()
        resume = self._call_llm(
            system_prompt=draft_prompt,
            user_prompt=f"ã€ç›®æ ‡ JDã€‘:\n{jd_text}\n\nã€æˆ‘çš„ä¹±éº»æ€ç»ªã€‘:\n{raw_thoughts}",
            response_model=ResumeFull
        )
        
        # --- Phase 2: Critique ---
        critique_prompt = self.prompt_manager.get_critic_prompt()
        critique = self._call_llm(
            system_prompt=critique_prompt,
            user_prompt=f"ã€ç›®æ ‡ JDã€‘:\n{jd_text}\n\nã€ç”Ÿæˆçš„ç®€å†ã€‘:\n{resume.model_dump_json()}",
            response_model=ResumeCritique
        )
        
        # ä»…å½“è´¨é‡ä¸ä½³æ—¶æ‰æ˜¾ç¤ºè¯„åˆ†å’Œæ„è§ï¼Œé¿å…å™ªéŸ³
        if critique.score < 90:
            self._print_status(f"ğŸ§ ç®€å†åˆè¯„: {critique.score}åˆ†", "\033[93m") # é»„è‰²è­¦å‘Š
            if critique.needs_revision:
                self._print_status(f"ğŸ’¡ ä¼˜åŒ–å»ºè®®: {critique.critique[:100]}...", "\033[90m") # ç°è‰²è¯¦æƒ…

        # --- Phase 3: Refine ---
        if critique.needs_revision and critique.score < 90:
            self._print_status("âœ¨ æ­£åœ¨æ ¹æ®æ„è§ç²¾ä¿®ç®€å†...", "\033[96m") # é’è‰²
            refine_prompt = self.prompt_manager.get_refine_prompt()
            resume = self._call_llm(
                system_prompt=refine_prompt,
                user_prompt=f"ã€ç›®æ ‡ JDã€‘:\n{jd_text}\n\nã€Critic æ„è§ã€‘:\n{critique.model_dump_json()}\n\nã€ç®€å†åˆç¨¿ã€‘:\n{resume.model_dump_json()}",
                response_model=ResumeFull
            )
        else:
            self._print_status("âœ… ç®€å†è´¨é‡è¾¾æ ‡ï¼Œç›´æ¥è¾“å‡ºã€‚", "\033[92m") # ç»¿è‰²

        return resume
